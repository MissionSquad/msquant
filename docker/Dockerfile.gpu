ARG BASE_IMAGE=nvcr.io/nvidia/pytorch:24.06-py3
FROM ${BASE_IMAGE}

ENV DEBIAN_FRONTEND=noninteractive \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PIP_NO_CACHE_DIR=1 \
    HF_HOME=/workspace/hf \
    HF_DATASETS_CACHE=/workspace/hf/datasets \
    HF_HUB_ENABLE_HF_TRANSFER=1 \
    PYTHONUNBUFFERED=1

# System packages (if needed, keep minimal)
RUN apt-get update && apt-get install -y --no-install-recommends ca-certificates && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* /var/cache/apt/archives/*

# Upgrade pip
RUN pip install --upgrade pip

# Install Python dependencies
RUN pip install "datasets"

RUN pip install "pandas" "huggingface_hub" "hf_transfer" "tqdm"

RUN pip install "accelerate" "safetensors"

RUN pip install "nicegui>=2.0.0" "nicegui-highcharts>=0.2.0" 
      
RUN pip install "vllm>=0.11.0" 

RUN pip install "transformers>=4.52.0"

RUN pip install "llmcompressor"

# Install llama.cpp for GGUF quantization support
ARG LLAMA_CPP_VERSION=b6945
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    unzip \
    && apt-get clean && \
    rm -rf /var/lib/apt/lists/* /var/cache/apt/archives/*

# Download and install pre-compiled llama.cpp binary
RUN curl -L -o /tmp/llama.zip https://github.com/ggml-org/llama.cpp/releases/download/${LLAMA_CPP_VERSION}/llama-${LLAMA_CPP_VERSION}-bin-ubuntu-x64.zip && \
    unzip -q /tmp/llama.zip -d /tmp/llama-extract && \
    mv /tmp/llama-extract/build /opt/llama.cpp && \
    rm -rf /tmp/llama.zip /tmp/llama-extract && \
    chmod +x /opt/llama.cpp/bin/llama-* && \
    pip install gguf

# Add llama.cpp to PATH
ENV PATH="/opt/llama.cpp:${PATH}"

WORKDIR /workspace

# Copy source
# Assuming our code is in ./src/msquant. Adjust if different.
COPY src /workspace/src
ENV PYTHONPATH=/workspace/src

# Expose NiceGUI port
EXPOSE 8080

# Entrypoint
CMD ["python", "-m", "msquant.app.main"]
